{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCziByBJwK+E1ghEgHbbQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpsuen/Deep_Learning_Data/blob/main/script/MiniGPT_example01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32yj22yvyFHn",
        "outputId": "49bcaf09-aa7b-4450-861e-1fb8c3414f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 18:11:15--  https://www.gutenberg.org/cache/epub/1504/pg1504.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112380 (110K) [text/plain]\n",
            "Saving to: â€˜corpus.txtâ€™\n",
            "\n",
            "corpus.txt          100%[===================>] 109.75K   647KB/s    in 0.2s    \n",
            "\n",
            "2025-07-19 18:11:15 (647 KB/s) - â€˜corpus.txtâ€™ saved [112380/112380]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/1504/pg1504.txt -O corpus.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "# â€” Config â€”\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size, block_size = 16, 64\n",
        "max_iters, eval_interval = 500, 100\n",
        "lr, n_embd, n_head, n_layer = 1e-3, 128, 4, 2\n",
        "dropout = 0.1\n"
      ],
      "metadata": {
        "id": "jljN_6TF386w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Load corpus â€”\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "encode = lambda s: [stoi[ch] for ch in s if ch in stoi]\n",
        "decode = lambda idxs: ''.join(itos[i] for i in idxs)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "x, y = data[:-1], data[1:]\n",
        "\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(x)-block_size, (batch_size,))\n",
        "    xb = torch.stack([x[i:i+block_size] for i in ix]).to(device)\n",
        "    yb = torch.stack([y[i:i+block_size] for i in ix]).to(device)\n",
        "    return xb, yb"
      ],
      "metadata": {
        "id": "Ba-LB-so4Hc_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Model definition â€”\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.size()\n",
        "        k = self.key(x); q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.sa = MultiHead()\n",
        "        self.ff = FeedForward()\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MiniLLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.size()\n",
        "        tok = self.tok_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=device))\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new):\n",
        "        for _ in range(max_new):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            probs = F.softmax(logits[:, -1], dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "VVcBlXF94S9V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Train & sample â€”\n",
        "model = MiniLLM().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for it in range(max_iters):\n",
        "    xb, yb = get_batch()\n",
        "    logits, loss = model(xb, yb)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    if it % eval_interval == 0:\n",
        "        print(f\"It {it} | Loss {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vhx53oG4pm6",
        "outputId": "56dfa1df-95d3-4e2f-dc30-3354898e65e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 0 | Loss 4.6396\n",
            "It 100 | Loss 2.4248\n",
            "It 200 | Loss 2.4056\n",
            "It 300 | Loss 2.3108\n",
            "It 400 | Loss 2.1605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Generate some text â€”\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "out = model.generate(context, max_new=300)\n",
        "print(decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpd8qQ0A5DeC",
        "outputId": "9c9c3ece-1455-4000-9b80-6f407154a004"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mandeme thureat, aingy shoun;\n",
            "On no, an hid got bbe hat Intempash wicee ad ut sececont anema soffork what de; shancyat, whes me forkâ€™d thathe whe chour, romahestraun thool hatest, pot ane rasdrar argin your Dree cio teswe watior to dichs o wh disst.\n",
            "U thrive por nok wor my het outh, potus, shâ€™stelid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Interactive generation loop ---\n",
        "print(\"\\nðŸŽ­ MiniLLM Interactive Mode (type 'exit' to quit)\")\n",
        "while True:\n",
        "    prompt = input(\"\\nYou > \").strip()\n",
        "    if prompt.lower() in ['exit', 'quit']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    if not prompt:\n",
        "        continue\n",
        "    try:\n",
        "        context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "    except KeyError as e:\n",
        "        print(f\"âš ï¸ Unknown character in input: {e}\")\n",
        "        continue\n",
        "    out = model.generate(context, max_new=300)[0]\n",
        "    result = decode(out.tolist())\n",
        "    print(\"\\nMiniLLM > \" + result[len(prompt):])  # Show only generated part\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6IC-hB5jU4",
        "outputId": "dbb611f7-a29c-4a30-ea80-fc7e73057145"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ­ MiniLLM Interactive Mode (type 'exit' to quit)\n",
            "\n",
            "You > Good sir, where is Antipholus?\n",
            "\n",
            "MiniLLM > \n",
            "Fentere, alis wer. Shurer, da se, fen speartiond thald thies.\n",
            "\n",
            "DUCIOREO.\n",
            "Is th shold vomale fer he inecoutyourtin ratâ€™denâ€”of 1starmichus ou bentith wer,\n",
            " a inc me je th to1. IPHIOLUS OF.YRACUSE.\n",
            " ANALAnd sleyou ppon my, you meataswe to thait,\n",
            "ANBBBy broby ofny toatrabridnd may thttorn, e andaerate\n",
            "\n",
            "\n",
            "You > exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}