{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbkRR13eDw+cE6T++yLh9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snpsuen/Deep_Learning_Data/blob/main/script/MiniGPT_example01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32yj22yvyFHn",
        "outputId": "3f5cce96-61dc-4c51-9de8-2ed523c1f16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 18:34:57--  https://www.gutenberg.org/cache/epub/1504/pg1504.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112380 (110K) [text/plain]\n",
            "Saving to: â€˜corpus.txtâ€™\n",
            "\n",
            "corpus.txt          100%[===================>] 109.75K   719KB/s    in 0.2s    \n",
            "\n",
            "2025-07-19 18:34:57 (719 KB/s) - â€˜corpus.txtâ€™ saved [112380/112380]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# â€” Read corpus â€”\n",
        "!wget https://www.gutenberg.org/cache/epub/1504/pg1504.txt -O corpus.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dZAZOG569lxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "# â€” Config â€”\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size, block_size = 16, 64\n",
        "max_iters, eval_interval = 500, 100\n",
        "lr, n_embd, n_head, n_layer = 1e-3, 128, 4, 2\n",
        "dropout = 0.1\n"
      ],
      "metadata": {
        "id": "jljN_6TF386w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Load corpus â€”\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "encode = lambda s: [stoi[ch] for ch in s if ch in stoi]\n",
        "decode = lambda idxs: ''.join(itos[i] for i in idxs)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "x, y = data[:-1], data[1:]\n",
        "\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(x)-block_size, (batch_size,))\n",
        "    xb = torch.stack([x[i:i+block_size] for i in ix]).to(device)\n",
        "    yb = torch.stack([y[i:i+block_size] for i in ix]).to(device)\n",
        "    return xb, yb"
      ],
      "metadata": {
        "id": "Ba-LB-so4Hc_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Model definition â€”\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.size()\n",
        "        k = self.key(x); q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.sa = MultiHead()\n",
        "        self.ff = FeedForward()\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MiniLLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.size()\n",
        "        tok = self.tok_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=device))\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new):\n",
        "        for _ in range(max_new):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            probs = F.softmax(logits[:, -1], dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "VVcBlXF94S9V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Train & sample â€”\n",
        "model = MiniLLM().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for it in range(max_iters):\n",
        "    xb, yb = get_batch()\n",
        "    logits, loss = model(xb, yb)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    if it % eval_interval == 0:\n",
        "        print(f\"It {it} | Loss {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vhx53oG4pm6",
        "outputId": "0fc69cb4-bd29-4ea0-a1ff-b5bfe142dc14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 0 | Loss 4.6514\n",
            "It 100 | Loss 2.6050\n",
            "It 200 | Loss 2.2844\n",
            "It 300 | Loss 2.1604\n",
            "It 400 | Loss 2.3240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â€” Generate some text â€”\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "out = model.generate(context, max_new=300)\n",
        "print(decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpd8qQ0A5DeC",
        "outputId": "2c8947d6-365a-44a6-c554-a01c5c8b3807"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " OF RACEMISE.\n",
            "And lever ware se, and mon will, ge me ar wond cove, ing walff a jald airk th\n",
            "le thanve napt thes alaven my in ar ine to sow.\n",
            "\n",
            "ANThain Af cor E.\n",
            "Tor i .\n",
            "\n",
            "Haicend he yoâ€™so wat Fort way,lok; fful wwh.t, sher I ingar\n",
            "of if an eve mageNe.\n",
            "\n",
            "ANTIPHOLUS OF 9 nand I EPHESUK.\n",
            " nont I.\n",
            "Tor ver c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Interactive generation loop ---\n",
        "print(\"\\nðŸŽ­ MiniLLM Interactive Mode (type 'exit' to quit)\")\n",
        "while True:\n",
        "    prompt = input(\"\\nYou > \").strip()\n",
        "    if prompt.lower() in ['exit', 'quit']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    if not prompt:\n",
        "        continue\n",
        "    try:\n",
        "        context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "    except KeyError as e:\n",
        "        print(f\"âš ï¸ Unknown character in input: {e}\")\n",
        "        continue\n",
        "    out = model.generate(context, max_new=300)[0]\n",
        "    result = decode(out.tolist())\n",
        "    print(\"\\nMiniLLM > \" + result[len(prompt):])  # Show only generated part\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6IC-hB5jU4",
        "outputId": "6d471713-e551-4d8d-9f67-a0be9d72f28c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ­ MiniLLM Interactive Mode (type 'exit' to quit)\n",
            "\n",
            "You > DUKE:\n",
            "\n",
            "MiniLLM > \n",
            "EN.\n",
            "\n",
            "DRONHESE.\n",
            "My Istre ics, hantus and ouiked,\n",
            "\n",
            "Dan, tio [_Enasthe co hak.Exun of anc?\n",
            "Whepear hob? And that se be andâ€™y.\n",
            "\n",
            "\n",
            "Thesm Terestannbe dach thanout  el he.\n",
            "\n",
            "Tho EPHOLUSy I To ingont, Un the ondea,\n",
            "I yo car yot may youghy, shve ilct?\n",
            "\n",
            "DROMIO Oll SERYRmat the st gourke fe rorke fors.\n",
            "\n",
            "Dy End \n",
            "\n",
            "You > What is the matter, sir? I know you not:\n",
            "\n",
            "MiniLLM > \n",
            "I wanthe hayner mping hobe and eve in.\n",
            "Whestior th you co muskee min,\n",
            " hald   on foree (mse ronngoordest; totâ€™ to f you alritfin a fardd he uct.\n",
            "Thy wod murgh  And sof wold jevand bonka dide.\n",
            "It be, he wit wist patence in ang ove,\n",
            "Thernd fond of me me to thago Ep. Lastacaver hour. Or and cait?\n",
            "\n",
            "We \n",
            "\n",
            "You > Good sir, where is Antipholus?\n",
            "\n",
            "MiniLLM > \n",
            "Bunc ch meed\n",
            "Tis, a and it and And alt wer me with me,\n",
            "Epy thery, rown wIâ€™ser bre Tit hif o grighiste, ge yed me,\n",
            "Wely nowal will wor colly hasearn to nordibkern was hy burkyâ€™d-buvimâ€™st if,\n",
            "comir gonds my canson Ifgomm uf\n",
            "Sivimâ€™st eank orsien, angay band hak Py,\n",
            "Thatllar bandot b mor. Terof hemate \n",
            "\n",
            "You > exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}